import os
import subprocess
import pickle
import sqlite3
import yaml
from flask import Flask, request, jsonify
import markdown
import xml.etree.ElementTree as ET

app = Flask(__name__)

# ======================
# Database Configuration
# ======================
def init_db():
    conn = sqlite3.connect('llm_users.db')
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY,
            username TEXT,
            api_key TEXT,
            query_history TEXT
        )
    ''')
    conn.commit()
    conn.close()

init_db()

# ======================
# Vulnerable API Endpoints
# ======================

# 1. SQL Injection Vulnerability
@app.route('/llm/query', methods=['POST'])
def llm_query():
    """Process user query through LLM with SQL injection vulnerability"""
    data = request.json
    user_id = data.get('user_id')
    query = data.get('query')
    
    conn = sqlite3.connect('llm_users.db')
    cursor = conn.cursor()
    
    # UNSAFE: Direct string interpolation
    cursor.execute(f"SELECT api_key FROM users WHERE id = {user_id}")
    api_key = cursor.fetchone()[0]
    
    # UNSAFE: Pass query directly to LLM without sanitization
    response = subprocess.check_output(
        f'curl -X POST https://api.llm-service.com/v1/complete -H "Authorization: Bearer {api_key}" -d \'{{"prompt":"{query}"}}\'',
        shell=True
    )
    
    # UNSAFE: Log unsanitized query
    cursor.execute(f"UPDATE users SET query_history = query_history || '{query}' WHERE id = {user_id}")
    conn.commit()
    conn.close()
    
    return jsonify(json.loads(response))

# 2. Command Injection Vulnerability
@app.route('/llm/execute', methods=['POST'])
def llm_execute():
    """Execute LLM-generated code with command injection vulnerability"""
    data = request.json
    code = data.get('code')
    
    # UNSAFE: Direct execution of LLM-generated code
    result = subprocess.check_output(f"python -c '{code}'", shell=True)
    return result.decode('utf-8')

# 3. Unsafe Deserialization Vulnerability
@app.route('/llm/load_config', methods=['POST'])
def llm_load_config():
    """Load LLM configuration with pickle deserialization"""
    config_data = request.data
    
    # UNSAFE: Pickle deserialization
    config = pickle.loads(config_data)
    return jsonify({"config": str(config)})

# 4. XXE Injection Vulnerability
@app.route('/llm/parse_xml', methods=['POST'])
def llm_parse_xml():
    """Parse XML input from LLM with XXE vulnerability"""
    xml_data = request.data
    
    # UNSAFE: XML parsing with external entities
    root = ET.fromstring(xml_data)
    return jsonify({"root": root.tag})

# 5. Template Injection Vulnerability
@app.route('/llm/render', methods=['POST'])
def llm_render():
    """Render LLM output with template injection"""
    content = request.json.get('content')
    
    # UNSAFE: Direct markdown rendering
    return markdown.markdown(content)

# 6. Path Traversal Vulnerability
@app.route('/llm/read_file', methods=['GET'])
def llm_read_file():
    """Read file based on LLM request with path traversal"""
    filename = request.args.get('filename')
    
    # UNSAFE: Direct file access
    with open(filename, 'r') as f:
        return f.read()

# 7. Insecure YAML Loading
@app.route('/llm/load_yaml', methods=['POST'])
def llm_load_yaml():
    """Load YAML config from LLM with unsafe loader"""
    yaml_data = request.data
    
    # UNSAFE: Full YAML loading
    config = yaml.full_load(yaml_data)
    return jsonify(config)

# 8. SSRF Vulnerability
@app.route('/llm/fetch_url', methods=['GET'])
def llm_fetch_url():
    """Fetch URL based on LLM request with SSRF"""
    url = request.args.get('url')
    
    # UNSAFE: Direct URL fetching
    return subprocess.check_output(f"curl {url}", shell=True)

# ======================
# Test Cases
# ======================

def test_sql_injection():
    """SQL Injection Test"""
    payload = {
        "user_id": "1; DROP TABLE users;--",
        "query": "normal query"
    }
    return json.dumps(payload)

def test_command_injection():
    """Command Injection Test"""
    payload = {
        "code": "import os; os.system('rm -rf /')"
    }
    return json.dumps(payload)

def test_deserialization():
    """Deserialization Test"""
    class Evil:
        def __reduce__(self):
            return (os.system, ('rm -rf /',))
    return pickle.dumps(Evil())

def test_xxe_injection():
    """XXE Injection Test"""
    return '''<!DOCTYPE foo [ <!ENTITY xxe SYSTEM "file:///etc/passwd"> ]>
    <foo>&xxe;</foo>'''

def test_template_injection():
    """Template Injection Test"""
    return {
        "content": "[Click me](javascript:alert('XSS'))"
    }

def test_path_traversal():
    """Path Traversal Test"""
    return {
        "filename": "../../../etc/passwd"
    }

def test_yaml_unsafe_load():
    """YAML Unsafe Load Test"""
    return "!!python/object/apply:os.system ['rm -rf /']"

def test_ssrf():
    """SSRF Test"""
    return {
        "url": "file:///etc/passwd"
    }

if __name__ == '__main__':
    app.run(debug=True)
